{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruisong4/CS598-DLH-Group-103/blob/main/CS598_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmHMfxLu293l"
      },
      "source": [
        "Loading the library we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYn5c50v0cB1",
        "outputId": "efe3c437-f4b8-4085-cf55-94047ea0de76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, random_split, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q1VKrN9BNVm"
      },
      "source": [
        "Setup GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-WaMX-HBMsE",
        "outputId": "a515ada6-5824-4340-fdfe-85010c50c045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj_YOSA-7uT2"
      },
      "source": [
        "Setup the working dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIurNu8l7zbt"
      },
      "outputs": [],
      "source": [
        "#depending on your system and computer\n",
        "drive.mount('/content/drive/')\n",
        "os.chdir(\"/content/drive/My Drive/CS598_DATA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8qhV0Qvbd2b"
      },
      "source": [
        "Load word2vec from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDBZY448bjeF",
        "outputId": "cd127202-d004-4882-8544-cbd01a48cd93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec Model Loaded\n"
          ]
        }
      ],
      "source": [
        "# load the binary file\n",
        "model = KeyedVectors.load_word2vec_format(\"PubMed-w2v.bin\", binary=True)\n",
        "\n",
        "# create a util class to handle words not in model\n",
        "class W2V:\n",
        "  def __init__(self, model=None):\n",
        "    self.w2v = model\n",
        "    self.embedding_size = self.w2v.vector_size\n",
        "    self.unknow_words = dict()\n",
        "  def __getitem__(self, key):\n",
        "    if key in self.w2v:\n",
        "      return self.w2v[key]\n",
        "    if key not in self.unknow_words:\n",
        "      self.unknow_words[key] = np.random.uniform(-1, 1, (self.embedding_size,))\n",
        "    return self.unknow_words.get(key)\n",
        "\n",
        "w2v = W2V(model)\n",
        "print(\"Word2Vec Model Loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buIjqCAii2aa"
      },
      "source": [
        "load the data from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn5-kY-acGEQ",
        "outputId": "555e0b0f-d60b-43d1-b22a-1eb196bb2146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data File Loaded\n"
          ]
        }
      ],
      "source": [
        "x_general = []\n",
        "x_thirty_days = []\n",
        "y_general = []\n",
        "y_thirty_days = []\n",
        "\n",
        "with open('x.txt') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "      x_general.append(line)\n",
        "      line = f.readline()\n",
        "\n",
        "with open('x_30.txt') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "      x_thirty_days.append(line)\n",
        "      line = f.readline()\n",
        "\n",
        "with open('y.txt') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "      y_general.append(int(line))\n",
        "      line = f.readline()\n",
        "\n",
        "with open('y_30.txt') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "      y_thirty_days.append(int(line))\n",
        "      line = f.readline()\n",
        "\n",
        "y_general = np.array(y_general)\n",
        "y_thirty_days = np.array(y_thirty_days)\n",
        "\n",
        "# sanity check\n",
        "assert len(x_general) == len(y_general)\n",
        "assert len(x_thirty_days) == len(y_thirty_days)\n",
        "assert y_general.max() == 1 and y_general.min() == 0\n",
        "assert y_thirty_days.max() == 1 and y_thirty_days.min() == 0\n",
        "print(\"Data File Loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_H2kKaFOVOF"
      },
      "source": [
        "tokenize the discharge note into words and calculating the max document length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J0Kq6lHObpi",
        "outputId": "1e4d0010-e57d-4550-b144-06dfc5ef7417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish tokenizing all words\n",
            "In general dataset the max word count: 4115 min count: 13 avg: 1196.8879310344828\n",
            "In 30-days dataset the max word count: 3553 min count: 21 avg: 1224.2037845705968\n"
          ]
        }
      ],
      "source": [
        "x_general_tokenized = []\n",
        "x_thirty_days_tokenized = []\n",
        "x_general_max_words = 0\n",
        "x_thirty_days_max_words = 0\n",
        "x_general_min_words = np.inf\n",
        "x_thirty_days_min_words = np.inf\n",
        "x_general_avg_words = 0\n",
        "x_thirty_days_avg_words = 0\n",
        "\n",
        "for note in x_general:\n",
        "  words = nltk.word_tokenize(note)\n",
        "  words = [word for word in words if word.isalnum()]\n",
        "  x_general_tokenized.append(words)\n",
        "  x_general_max_words = max(x_general_max_words, len(words))\n",
        "  x_general_min_words = min(x_general_min_words, len(words))\n",
        "  x_general_avg_words += len(words)\n",
        "x_general_avg_words /= len(x_general_tokenized)\n",
        "\n",
        "for note in x_thirty_days:\n",
        "  words = nltk.word_tokenize(note)\n",
        "  words = [word for word in words if word.isalnum()]\n",
        "  x_thirty_days_tokenized.append(words)\n",
        "  x_thirty_days_max_words = max(x_thirty_days_max_words, len(words))\n",
        "  x_thirty_days_min_words = min(x_thirty_days_min_words, len(words))\n",
        "  x_thirty_days_avg_words += len(words)\n",
        "x_thirty_days_avg_words /= len(x_thirty_days_tokenized)\n",
        "\n",
        "print(\"Finish tokenizing all words\")\n",
        "print(\"In general dataset the max word count:\", x_general_max_words, \"min count:\", x_general_min_words, \"avg:\", x_general_avg_words)\n",
        "print(\"In 30-days dataset the max word count:\", x_thirty_days_max_words, \"min count:\", x_thirty_days_min_words, \"avg:\", x_thirty_days_avg_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUmJqooEnZO4"
      },
      "source": [
        "1.   Create DataSet to hold the data\n",
        "2.   Generarte train and val dataset (90% train 10% val, accodring to the paper)\n",
        "3.   Creating DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeLdQFq3n_8l"
      },
      "outputs": [],
      "source": [
        "class ReadmissionDataSet(Dataset):\n",
        "  def __init__(self, notes, labels, w2v, max_len):\n",
        "    self.x = notes\n",
        "    self.y = labels\n",
        "    self.max_len = max_len\n",
        "    self.w2v = w2v\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    note = np.array([self.w2v[w] for w in self.x[index]], dtype=np.float32)\n",
        "    padded = note\n",
        "    if (len(note) < self.max_len):\n",
        "      pad = np.zeros((self.max_len - len(note), w2v.embedding_size,), dtype=np.float32)\n",
        "      padded = np.concatenate([padded, pad])\n",
        "    return padded, self.y[index]\n",
        "\n",
        "general_data_set = ReadmissionDataSet(x_general_tokenized, y_general, w2v, x_general_max_words)\n",
        "thirty_days_data_set = ReadmissionDataSet(x_thirty_days_tokenized, y_thirty_days, w2v, x_thirty_days_max_words)\n",
        "\n",
        "general_training_size = int(0.9 * len(general_data_set))\n",
        "thirty_days_training_size = int(0.9 * len(thirty_days_data_set))\n",
        "\n",
        "general_test_size = len(general_data_set) - general_training_size\n",
        "thirty_days_test_size = len(thirty_days_data_set) - thirty_days_training_size\n",
        "\n",
        "general_train_dataset, general_test_dataset = random_split(general_data_set, [general_training_size, general_test_size])\n",
        "thirty_days_train_dataset, thirty_days_test_dataset = random_split(thirty_days_data_set, [thirty_days_training_size, thirty_days_test_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sur6iwuBq94C"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y_EFQx0q8s1"
      },
      "outputs": [],
      "source": [
        "class ReadmissionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ReadmissionModel, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, 200))\n",
        "    self.conv2 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(2, 200))\n",
        "    self.conv3 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 200)) \n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "    self.linear = nn.Linear(64 * 3, 2)\n",
        "    \n",
        "  def forward(self, x):\n",
        "\n",
        "    unsqueezed = torch.unsqueeze(x, 1)\n",
        "\n",
        "    con1_out = self.conv1(unsqueezed)\n",
        "    con1_out = torch.squeeze(con1_out, dim=3)\n",
        "    con1_out = F.relu(con1_out)\n",
        "    con1_out = F.max_pool1d(con1_out, kernel_size=con1_out.shape[2])\n",
        "    con1_out = torch.squeeze(con1_out, dim=2)\n",
        "\n",
        "    con2_out = self.conv2(unsqueezed)\n",
        "    con2_out = torch.squeeze(con2_out, dim=3)\n",
        "    con2_out = F.relu(con2_out)\n",
        "    con2_out = F.max_pool1d(con2_out, kernel_size=con2_out.shape[2])\n",
        "    con2_out = torch.squeeze(con2_out, dim=2)\n",
        "\n",
        "    con3_out = self.conv3(unsqueezed)\n",
        "    con3_out = torch.squeeze(con3_out, dim=3)\n",
        "    con3_out = F.relu(con3_out)\n",
        "    con3_out = F.max_pool1d(con3_out, kernel_size=con3_out.shape[2])\n",
        "    con3_out = torch.squeeze(con3_out, dim=2)\n",
        "\n",
        "    out = torch.cat((con1_out, con2_out, con3_out), dim=1)\n",
        "    out = self.dropout(out)\n",
        "    out = self.linear(out)\n",
        "\n",
        "    #print(\"output size:\",torch.squeeze(self.linear(out)).shape)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper function to calculate accuracy"
      ],
      "metadata": {
        "id": "wys7ZRtrjO7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_g_lFNFZVeu"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.argmax(dim=1)\n",
        "    correct = (preds == labels).sum().float()\n",
        "    acc = correct / len(labels)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OsVTipu9UFa"
      },
      "source": [
        "Helper function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jKLeojM9TSW"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, n_epoch, optimizer, criterion, device):\n",
        "  model.train()\n",
        "  m = nn.LogSoftmax(dim=1)\n",
        "  for epoch in range(n_epoch):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for data, target in train_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      y_hat = model(data)\n",
        "      acc = accuracy(y_hat, target)\n",
        "\n",
        "      loss = criterion(y_hat, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "    print(f\"Epoch {epoch}: loss: {epoch_loss / len(train_loader)} acc: {100*epoch_acc / len(train_loader)}\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV2rw9VHCZ4q"
      },
      "source": [
        "Helper function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sBvzJTTCcoO"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader):\n",
        "  model.eval()\n",
        "  Y_pred = []\n",
        "  Y_test = []\n",
        "\n",
        "  for data, target in test_loader:\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    Y_test.extend(target.tolist())\n",
        "    y_hat = model(data)\n",
        "    y_hat = y_hat.argmax(dim=1)\n",
        "    Y_pred.extend(y_hat.tolist())\n",
        "\n",
        "  Y_test = np.array(Y_test)\n",
        "\n",
        "  return Y_pred, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua0FA-6ZW1Ag"
      },
      "source": [
        "define collate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlfKmHHW0s6"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "  return torch.cat([torch.unsqueeze(torch.from_numpy(x[0]), 0) for x in data], dim=0).float(), torch.tensor([x[1] for x in data], dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyWV6dFm5big"
      },
      "source": [
        "10-fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opqBPbAFAiWt"
      },
      "outputs": [],
      "source": [
        "def cross_validate(model, dataset, n_splits, batch_size, n_epoch, optimizer, criterion, device):\n",
        "\n",
        "  kfold = KFold(n_splits=n_splits, shuffle=True, random_state=598)\n",
        "\n",
        "  model_performance = []\n",
        "\n",
        "  for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
        "    print(\"Fold\", fold, \"begins\")\n",
        "    train_subsampler = SubsetRandomSampler(train_idx)\n",
        "    test_subsampler = SubsetRandomSampler(test_idx)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_subsampler, collate_fn=collate_fn)\n",
        "    \n",
        "    # clear model weight for next fold\n",
        "    count = 0\n",
        "    for layer in model.children():\n",
        "      if hasattr(layer, \"reset_parameters\"):\n",
        "        count += 1\n",
        "        layer.reset_parameters()\n",
        "    print(\"resetting weight in\", count, \"layers\")\n",
        "\n",
        "    train_model(model, train_loader, n_epoch, optimizer, criterion, device)\n",
        "    \n",
        "    Y_pred, Y_test = test_model(model, test_loader)\n",
        "    acc = accuracy_score(Y_test, Y_pred)\n",
        "    p, r, f, _ = precision_recall_fscore_support(Y_test, Y_pred, average='binary')\n",
        "    print(\"Fold\", fold, \"results: \", \"percision:\", p, \"recall\", r, \"f1\", f, \"acc\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZrWDTx9FsmM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XN08VOLsPOQt",
        "outputId": "db9b8ccf-e20f-4f2a-f781-63d0674bce21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReadmissionModel(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
            "  (conv2): Conv2d(1, 64, kernel_size=(2, 200), stride=(1, 1))\n",
            "  (conv3): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (linear): Linear(in_features=192, out_features=2, bias=True)\n",
            ")\n",
            "Fold 0 begins\n",
            "resetting weight in 4 layers\n",
            "Epoch 0: loss: 0.7042130821353787 acc: 54.0521978021978\n",
            "Epoch 1: loss: 0.6712891590464246 acc: 58.567994505494504\n",
            "Epoch 2: loss: 0.6545111776053251 acc: 61.092032967032964\n",
            "Epoch 3: loss: 0.6336868588741009 acc: 64.02815934065934\n",
            "Epoch 4: loss: 0.621084199665667 acc: 65.43612637362638\n",
            "Epoch 5: loss: 0.6019299899811273 acc: 66.75824175824175\n",
            "Epoch 6: loss: 0.5831157776353123 acc: 68.9217032967033\n",
            "Epoch 7: loss: 0.5749684136647445 acc: 69.72870879120879\n",
            "Epoch 8: loss: 0.5544580802485183 acc: 71.17101648351648\n",
            "Epoch 9: loss: 0.5357003814571506 acc: 73.30013736263736\n",
            "Fold 0 results:  percision: 0.6198830409356725 recall 0.660436137071651 f1 0.6395173453996983 acc 0.6311728395061729\n",
            "Fold 1 begins\n",
            "resetting weight in 4 layers\n",
            "Epoch 0: loss: 0.7095231374541482 acc: 53.863324175824175\n",
            "Epoch 1: loss: 0.6735743850797087 acc: 58.43063186813187\n",
            "Epoch 2: loss: 0.6490314593026926 acc: 61.96771978021978\n",
            "Epoch 3: loss: 0.6309398442179293 acc: 63.65041208791209\n",
            "Epoch 4: loss: 0.6195582506748346 acc: 65.81387362637362\n",
            "Epoch 5: loss: 0.5988302650032463 acc: 67.20467032967034\n",
            "Epoch 6: loss: 0.5880932776810048 acc: 67.90865384615384\n",
            "Epoch 7: loss: 0.576769870879886 acc: 69.17925824175825\n",
            "Epoch 8: loss: 0.5554337226427518 acc: 71.54876373626374\n",
            "Epoch 9: loss: 0.5448602632834361 acc: 72.0467032967033\n",
            "Fold 1 results:  percision: 0.6056338028169014 recall 0.6574923547400612 f1 0.6304985337243402 acc 0.6111111111111112\n",
            "Fold 2 begins\n",
            "resetting weight in 4 layers\n",
            "Epoch 0: loss: 0.7065274142176727 acc: 54.40573770491803\n",
            "Epoch 1: loss: 0.6739971253389869 acc: 58.060109289617486\n",
            "Epoch 2: loss: 0.6532021653456767 acc: 60.62158469945355\n",
            "Epoch 3: loss: 0.641489645822452 acc: 62.568306010928964\n",
            "Epoch 4: loss: 0.6196130380604437 acc: 65.3688524590164\n",
            "Epoch 5: loss: 0.6133804588369984 acc: 65.9153005464481\n",
            "Epoch 6: loss: 0.5931902829089452 acc: 67.81079234972678\n",
            "Epoch 7: loss: 0.5718321950057816 acc: 70.06489071038251\n",
            "Epoch 8: loss: 0.5553003582960921 acc: 71.67008196721312\n",
            "Epoch 9: loss: 0.5395804926183054 acc: 72.30191256830601\n",
            "Fold 2 results:  percision: 0.7013888888888888 recall 0.6273291925465838 f1 0.6622950819672131 acc 0.6816074188562596\n",
            "Fold 3 begins\n",
            "resetting weight in 4 layers\n",
            "Epoch 0: loss: 0.7087183913897946 acc: 53.07377049180328\n",
            "Epoch 1: loss: 0.6713087509890072 acc: 58.96516393442623\n",
            "Epoch 2: loss: 0.648813356874419 acc: 62.10724043715847\n",
            "Epoch 3: loss: 0.6367962638862797 acc: 64.24180327868852\n",
            "Epoch 4: loss: 0.6128594440514924 acc: 66.63251366120218\n",
            "Epoch 5: loss: 0.5989085683405725 acc: 68.34016393442623\n",
            "Epoch 6: loss: 0.5886303267843737 acc: 67.65710382513662\n",
            "Epoch 7: loss: 0.5726206608808757 acc: 69.80874316939891\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-b264a6a00517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneral_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-82-76947ec3bc2f>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(model, dataset, n_splits, batch_size, n_epoch, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resetting weight in\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-929397eb8223>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, n_epoch, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mepoch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}: loss: {epoch_loss / len(train_loader)} acc: {100*epoch_acc / len(train_loader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = ReadmissionModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "criterion = criterion.to(device)\n",
        "cross_validate(model, general_train_dataset, 10, 32, 10, optimizer, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ReadmissionModel()\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6APDYaXci7K",
        "outputId": "6447a3a6-4b43-4bc1-8266-42996b478e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77378"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "CS598 Final Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}